{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soumil Nitin Shah \n",
    "* Bachelor in Electronic Engineering |\n",
    "* Masters in Electrical Engineering | \n",
    "* Master in Computer Engineering |\n",
    "\n",
    "* Website : https://soumilshah.herokuapp.com\n",
    "* Github: https://github.com/soumilshah1995\n",
    "* Linkedin: https://www.linkedin.com/in/shah-soumil/\n",
    "* Blog: https://soumilshah1995.blogspot.com/\n",
    "* Youtube : https://www.youtube.com/channel/UC_eOodxvwS_H7x2uLQa-svw?view_as=subscriber\n",
    "* Facebook Page : https://www.facebook.com/soumilshah1995/\n",
    "* Email : shahsoumil519@gmail.com\n",
    "* projects : https://soumilshah.herokuapp.com/project\n",
    "\n",
    "\n",
    "Hello! I’m Soumil Nitin Shah, a Software and Hardware Developer based in New York City. I have completed by Bachelor in Electronic Engineering and my Double master’s in Computer and Electrical Engineering. I Develop Python Based Cross Platform Desktop Application , Webpages , Software, REST API, Database and much more I have more than 2 Years of Experience in Python\n",
    "\n",
    "\n",
    "## Authors\n",
    "\n",
    "* **Soumil Nitin Shah** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Pyspark in a easy way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\python38\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\python38\\lib\\site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "# Installing pyspark \n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JTSTDSSHAH2.jobtarget.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyProcess</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x143270e6b20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"MyProcess\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: understadning how to create a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"city\", \"type\", \"price\"]\n",
    "data   = map(lambda r: (r[0], r[1], float(r[2])),\n",
    "  map(lambda x: x.split(\",\"),\n",
    "    [\"Paris,Food,19.00\", \"Marseille,Clothing,12.00\",\n",
    "     \"Paris,Food,8.00\", \"Paris,Clothing,15.00\",\n",
    "     \"Marseille,Food,20.00\", \"Lyon,Book,10.00\"]))\n",
    "df = spark.createDataFrame(data, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|     city|    type|price|\n",
      "+---------+--------+-----+\n",
      "|    Paris|    Food| 19.0|\n",
      "|Marseille|Clothing| 12.0|\n",
      "|    Paris|    Food|  8.0|\n",
      "|    Paris|Clothing| 15.0|\n",
      "|Marseille|    Food| 20.0|\n",
      "|     Lyon|    Book| 10.0|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paris', 'Food', 19.0),\n",
       " ('Marseille', 'Clothing', 12.0),\n",
       " ('Paris', 'Food', 8.0),\n",
       " ('Paris', 'Clothing', 15.0),\n",
       " ('Marseille', 'Food', 20.0),\n",
       " ('Lyon', 'Book', 10.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data   = map(lambda r: (r[0], r[1], float(r[2])),\n",
    "  map(lambda x: x.split(\",\"),\n",
    "    [\"Paris,Food,19.00\", \"Marseille,Clothing,12.00\",\n",
    "     \"Paris,Food,8.00\", \"Paris,Clothing,15.00\",\n",
    "     \"Marseille,Food,20.00\", \"Lyon,Book,10.00\"]))\n",
    "list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Comments \n",
    "* One thing we understood is we can create a dataframe by defining a tuple inside a list this is a standard format \n",
    "* we also know that tuple is immutable that means we cannot change value inside tuple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are several methods to retrieve / display the contents of a dataframe:\n",
    "* .collect() method that retrieves the dataframe to the user as a python list.\n",
    "* .take(n) method that retrieves the « n » first elements of the dataframe.\n",
    "* .show() method that displays the contents on the standard output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic information about  Schema and hwo we can manipulate it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Paris', type='Food', price=19.0),\n",
       " Row(city='Marseille', type='Clothing', price=12.0),\n",
       " Row(city='Paris', type='Food', price=8.0),\n",
       " Row(city='Paris', type='Clothing', price=15.0),\n",
       " Row(city='Marseille', type='Food', price=20.0),\n",
       " Row(city='Lyon', type='Book', price=10.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(city='Paris', type='Food', price=19.0),\n",
       " Row(city='Marseille', type='Clothing', price=12.0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('city', 'string'), ('type', 'string'), ('price', 'double')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|     city|    type|price|\n",
      "+---------+--------+-----+\n",
      "|    Paris|    Food| 19.0|\n",
      "|Marseille|Clothing| 12.0|\n",
      "|    Paris|    Food|  8.0|\n",
      "|    Paris|Clothing| 15.0|\n",
      "|Marseille|    Food| 20.0|\n",
      "|     Lyon|    Book| 10.0|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
    "data = map(lambda r: (r[0], r[1], float(r[2])),\n",
    "  map(lambda x: x.split(\",\"),\n",
    "    [\"Paris,Food,19.00\", \"Marseille,Clothing,12.00\",\n",
    "     \"Paris,Food,8.00\", \"Paris,Clothing,15.00\",\n",
    "     \"Marseille,Food,20.00\", \"Lyon,Book,10.00\"]))\n",
    "\n",
    "schema = StructType([\n",
    "\tStructField(\"city\",  StringType(), nullable=True),\n",
    "\tStructField(\"type\",  StringType(), nullable=True),\n",
    "\tStructField(\"price\", FloatType(),  nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion \n",
    "* we learned how to create a datagframe and how we can also specify the datatypes of dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting Columns in PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     city|\n",
      "+---------+\n",
      "|    Paris|\n",
      "|Marseille|\n",
      "|    Paris|\n",
      "|    Paris|\n",
      "|Marseille|\n",
      "|     Lyon|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|     city|    type|\n",
      "+---------+--------+\n",
      "|    Paris|    Food|\n",
      "|Marseille|Clothing|\n",
      "+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting Multiple Columns \n",
    "df.select([\"city\", \"type\"]).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter commands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+\n",
      "| city|    type|price|\n",
      "+-----+--------+-----+\n",
      "|Paris|    Food| 19.0|\n",
      "|Paris|    Food|  8.0|\n",
      "|Paris|Clothing| 15.0|\n",
      "+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.city == \"Paris\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| city|type|price|\n",
      "+-----+----+-----+\n",
      "|Paris|Food| 19.0|\n",
      "|Paris|Food|  8.0|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.city == \"Paris\").filter(df.type == \"Food\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| city|type|price|\n",
      "+-----+----+-----+\n",
      "|Paris|Food| 19.0|\n",
      "|Paris|Food|  8.0|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\n",
    "    (df.city == \"Paris\") & (df.type == \"Food\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| city|\n",
      "+-----+\n",
      "|Paris|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\n",
    "    (df.city == \"Paris\") & (df.price > 18.0) ).select('city').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|  8.0|\n",
      "| 10.0|\n",
      "| 12.0|\n",
      "| 15.0|\n",
      "| 19.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.price < 20 ).orderBy(df.price.asc()).select(\"price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|  8.0|\n",
      "| 10.0|\n",
      "| 12.0|\n",
      "| 15.0|\n",
      "| 19.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.price < 20 ).sort(df.price.asc()).select(\"price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+---+\n",
      "|     city|    type|price|six|\n",
      "+---------+--------+-----+---+\n",
      "|    Paris|    Food| 19.0|  6|\n",
      "|Marseille|Clothing| 12.0|  6|\n",
      "|    Paris|    Food|  8.0|  6|\n",
      "|    Paris|Clothing| 15.0|  6|\n",
      "|Marseille|    Food| 20.0|  6|\n",
      "|     Lyon|    Book| 10.0|  6|\n",
      "+---------+--------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, rand\n",
    "df = df.withColumn(\"six\", lit(6)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-1a6ce2362cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+---------+\n",
      "|     city|    type|price|50percent|\n",
      "+---------+--------+-----+---------+\n",
      "|    Paris|    Food| 19.0|      9.5|\n",
      "|Marseille|Clothing| 12.0|      6.0|\n",
      "|    Paris|    Food|  8.0|      4.0|\n",
      "|    Paris|Clothing| 15.0|      7.5|\n",
      "|Marseille|    Food| 20.0|     10.0|\n",
      "|     Lyon|    Book| 10.0|      5.0|\n",
      "+---------+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, rand\n",
    "\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
    "\n",
    "data = map(lambda r: (r[0], r[1], float(r[2])),\n",
    "  map(lambda x: x.split(\",\"),\n",
    "    [\"Paris,Food,19.00\", \"Marseille,Clothing,12.00\",\n",
    "     \"Paris,Food,8.00\", \"Paris,Clothing,15.00\",\n",
    "     \"Marseille,Food,20.00\", \"Lyon,Book,10.00\"]))\n",
    "\n",
    "schema = StructType([\n",
    "\tStructField(\"city\",  StringType(), nullable=True),\n",
    "\tStructField(\"type\",  StringType(), nullable=True),\n",
    "\tStructField(\"price\", FloatType(),  nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df = df.withColumn(\"50percent\", df.price / 2).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     city|price|\n",
      "+---------+-----+\n",
      "|    Paris| 19.0|\n",
      "|Marseille| 12.0|\n",
      "|    Paris|  8.0|\n",
      "|    Paris| 15.0|\n",
      "|Marseille| 20.0|\n",
      "|     Lyon| 10.0|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(df.type).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|type|\n",
      "+----+\n",
      "|Food|\n",
      "|Food|\n",
      "|Food|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(*[\"city\", \"price\"]).filter(df.type == \"Food\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+\n",
      "|     CITY|    type|price|\n",
      "+---------+--------+-----+\n",
      "|    Paris|    Food| 19.0|\n",
      "|Marseille|Clothing| 12.0|\n",
      "|    Paris|    Food|  8.0|\n",
      "|    Paris|Clothing| 15.0|\n",
      "|Marseille|    Food| 20.0|\n",
      "|     Lyon|    Book| 10.0|\n",
      "+---------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"city\", \"CITY\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advanced Manipulations\n",
    "* In order to respond to most situations, PySpark already has a number of pre-implemented functions present in the pyspark.sql.functions module.\n",
    "\n",
    "* The functions are numerous and of all types. Do not hesitate to refer to it.\n",
    "* Mathematical functions (cos, sin, tan, abs, exp, log…).\n",
    "* Dataframes manipulation (concatenate, repartitioning, structure manipulation...).\n",
    "* Statistics (average, variance, correlation, covariance…).\n",
    "* Date manipulation.\n",
    "* Operators of relational algebra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+\n",
      "| x1| x2|            cos_sum|\n",
      "+---+---+-------------------+\n",
      "|  1|  2|-0.9899924966004454|\n",
      "|  2|  3|0.28366218546322625|\n",
      "|  3|  4| 0.7539022543433046|\n",
      "|  4|  5|-0.9111302618846769|\n",
      "+---+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cos\n",
    "df = spark.createDataFrame(\n",
    "  [[1, 2], [2, 3], [3, 4], [4, 5]],\n",
    "  schema=[\"x1\", \"x2\"])\n",
    "df.withColumn(\"cos_sum\", cos(df.x1 + df.x2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Group by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|State Count|\n",
      "+-----------+\n",
      "|          4|\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\").count().withColumnRenamed(\"count\", \"State Count\").select(\"State Count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+----------+\n",
      "|state|avg(salary)|avg(age)|avg(bonus)|\n",
      "+-----+-----------+--------+----------+\n",
      "|   CA|    87500.0|   29.75|   22000.0|\n",
      "|   NY|    85800.0|    45.8|   17000.0|\n",
      "+-----+-----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\").mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### joins in PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
